import logging
import os
import re
import shutil
import traceback
import git
import json
import tiktoken
from json import JSONDecodeError
import ast
import textwrap
import requests
import pdfplumber
import tempfile
from datetime import datetime

from langchain_openai import ChatOpenAI, OpenAIEmbeddings, AzureChatOpenAI, AzureOpenAIEmbeddings
from langgraph.graph import StateGraph, END
from typing import List, Optional, Union, Tuple
from pydantic import BaseModel, Field, ValidationError
from typing import List, Dict, TypedDict, Optional, Any
from langchain_core.documents import Document
from langchain_community.document_loaders import GitLoader, RecursiveUrlLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import json
import pandas as pd
import gdown
import numpy as np

from io import BytesIO
from PIL import Image
from typing import Union, List

from pptx import Presentation
from pptx.util import Pt
from pptx.dml.color import RGBColor


# Configure root logger
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
logging.getLogger("pdfminer.pdfpage").setLevel(logging.ERROR)


class ModelCard(BaseModel):
    """
    Pydantic model defining standardized metadata fields for a scientific software,
    model, dataset, or tool. Each field includes a default value and description
    to support automated or manual generation of model cards.
    """
    capability_name: str = Field(default="N/A", description="The primary name of the software, model, dataset, or tool.")
    brief_description: str = Field(default="N/A", description="A brief, one or two-sentence description of the software's main purpose and function.  What are the typical uses?  Who is currently using it?")
    systems_covered: str = Field(default="N/A", description="The real-world systems the model represents (e.g., energy systems, water resources, climate, ecosystems, land use, socioeconomics, machine learning models). List the main ones.")
    contact_name: str = Field(default="N/A", description="The name of the primary contact person for the software, if specified.")
    contact_email: str = Field(default="N/A", description="The email address of the primary contact person.")
    key_contributors: Union[List[str], str] = Field(default="N/A", description="Names or GitHub handles of other significant contributors mentioned.")
    doi: str = Field(default="N/A", description="The current Digital Object Identifier (DOI) for citing the software, if available (check CITATION.cff or README).")
    computational_requirements: Union[List[str], str] = Field(default="N/A", description="Hardware needed to run the software (e.g., Standard laptop, High-performance computing (HPC) cluster, GPU required).")
    sponsoring_projects: Union[List[str], str] = Field(default="N/A", description="Projects or funding agencies that sponsored the development, if mentioned.")
    figure: str = Field(default="N/A", description="Reference or URL to a key figure illustrating the software's structure or results, if described or linked in the text.")
    figure_caption: str = Field(default="N/A", description="The caption associated with the key figure, if available in the text.")
    spatial_resolution: Union[List[str], str] = Field(default="N/A", description="The spatial resolutions (range of tested grid spacing or distance between resolved features) at which the model can operate.  For example:  1 km, 5 arcmin, 0.5 degrees, Unstructured grid, Structured grid, Variable resolution, N/A if not applicable).  Return all that apply.")
    geographic_scope: Union[List[str], str] = Field(default="N/A", description="The geographic area the model covers or is applied to (e.g., Global, CONUS, specific region, point location). Mention both potential and current applications if specified.")
    temporal_resolution: Union[List[str], str] = Field(default="N/A", description="The time step or frequency of the model's calculations.  Either: seconds, minutes, hourly, daily, monthly, annual, 5-year, or decadal).  Return all that apply.")
    temporal_range: Union[List[str], str] = Field(default="N/A", description="The time period the model simulates.  Either: historical, future, specific period.  Return all that apply.")
    input_variables: Union[List[str], str] = Field(default="N/A", description="Key input data or variables required by the software.  List them.  Examples:  daily precipitation; power plant heat rate; locational marginal prices; gridded fractional land cover.")
    output_variables: Union[List[str], str] = Field(default="N/A", description="Key output data or results generated by the software. List them. Examples:  instantaneous cyclone energy; fractional land cover; monthly runoff; atmospheric radiation; daily precipitation.")
    interdependencies: Union[List[str], str] = Field(default="N/A", description="Other models, software libraries, or specific datasets the software relies on or commonly interacts with. For example, E3SM, GCAM, specific weather data).")
    key_publications: Union[List[str], str] = Field(default="N/A", description="List of key publications as found in the documentation or as provided as PDF files by the user.")
    category: Union[List[str], str] = Field(default="N/A", description="Select from:  Atmosphere, Physical Hydrology, Water Management, Wildfire, Energy, Multisectoral, Land Use Land Cover, Socioeconomics.  Return all that apply.")
    license: str = Field(default="N/A", description="The name of the license associated with the software, model, dataset, or tool.")
    current_version: str = Field(default="N/A", description="The current version of the software, model, dataset, or tool.")


class GraphState(TypedDict):
    """
    Represents the state of the graph as it progresses through document
    processing and model card generation.

    Attributes:
        input_urls (Dict[str, str]): A dictionary of input source URLs, keyed by source name.
        github_docs (Optional[List[Document]]): Documents retrieved from a GitHub repository.
        docs_docs (Optional[List[Document]]): Documents retrieved from a Google Docs source.
        extracted_info (Dict): Information extracted from the documents.
        model_card (Optional[Dict]): A dictionary representing the generated model card.
        error_messages (List[str]): A list of error messages encountered during processing.
        validation_issues (Dict): Validation issues detected during processing.
    """
    github_url: str
    github_branch: str
    github_content: Optional[List[Document]]
    docs_docs: Optional[List[Document]]
    publication_docs: Optional[List[Document]]
    extracted_info: Dict
    model_card: Optional[Dict]
    error_messages: List[str]
    validation_issues: Dict
    release_info: Dict
    github_release_errors: List[str]
    contributors_info: Dict
    github_contributor_errors: List[str]
    github_extracted_info: Dict
    github_repository_errors: List[str]
    handled_errors: List[str]
    llm_client: Any
    website_url: Optional[str]
    website_content: Optional[List[Document]]
    website_errors: List[str]
    website_extracted_info: Dict
    publication_directory: str
    publication_content: Optional[List[Document]]
    publication_extracted_info: Dict
    publication_errors: List[str]
    raw_extracted_info: Dict


def apply_text(
    slide_index: int,
    shape_index: int,
    text: str,
    font_name: str = "Calibri",
    font_size: int = 14,
    font_italic: bool = False,
    font_color = RGBColor(0, 0, 0), # black
    bold_items: Union[None, List[str]] = None,
) -> None:

    # retrieve shape
    shape = prs.slides[slide_index].shapes[shape_index]

    # write text to shape
    shape.text = text

    # apply formatting
    for paragraph in shape.text_frame.paragraphs:
        for run in paragraph.runs:
            run.font.name   = font_name
            run.font.size   = Pt(font_size)
            run.font.italic = font_italic
            run.font.color.rgb = font_color

            if bold_items is not None:
                for item in bold_items:
                    if run.text.startswith(item):
                        run.font.bold = True


def apply_figure(
    slide_index: int,
    shape_index: int,
    image_url: str,
) -> None:

    # download image & get its size
    resp = requests.get(image_url)
    resp.raise_for_status()
    img_bytes = BytesIO(resp.content)
    img = Image.open(img_bytes)
    orig_w, orig_h = img.size
    
    # open PPT and grab the target shape
    slide = prs.slides[slide_index]
    shape = slide.shapes[shape_index]
    
    # record the shape’s position & size
    left, top, w, h = shape.left, shape.top, shape.width, shape.height
    
    # compute scaling to fit entire image within the rectangle
    scale = min(w / orig_w, h / orig_h)
    scaled_w = int(orig_w * scale)
    scaled_h = int(orig_h * scale)
    
    # compute offsets to center the image in the rectangle
    offset_left = left + (w  - scaled_w) // 2
    offset_top = top  + (h  - scaled_h) // 2
    
    # insert the picture at the computed size & position
    img_bytes.seek(0)
    pic = slide.shapes.add_picture(
        img_bytes,
        offset_left,
        offset_top,
        width=scaled_w,
        height=scaled_h
    )
    
    # hide the original rectangle so only the image shows
    shape.fill.background()
    if shape.line:
        shape.line.fill.background()


def apply_table_cell(
    slide_index: int,
    shape_index: int,
    row_index: int,
    column_index: int,
    text: str,
    font_name: str = "Calibri",
    font_size: int = 14,
    font_italic: bool = False,
    font_color = RGBColor(0, 0, 0), # black
) -> None:

    # get shape object
    shape = prs.slides[slide_index].shapes[shape_index]

    # get cell
    cell = shape.table.cell(row_index, column_index)

    # apply text
    cell.text = text

    # apply font size to every run in that cell
    for paragraph in cell.text_frame.paragraphs:
        for run in paragraph.runs:
            run.font.name   = font_name
            run.font.size   = Pt(font_size)
            run.font.italic = font_italic
            run.font.color.rgb = font_color    


def download_gdrive_public_file(file_id_or_url, destination_path):
    """
    Downloads a publicly shared file from Google Drive using its ID or URL.

    Args:
        file_id_or_url (str): The Google Drive file ID or its sharable URL.
                              (e.g., '1ZdR3L3qP_qF2X...' or
                               'https://drive.google.com/file/d/1ZdR3L3qP_qF2X.../view?usp=sharing')
        destination_path (str): The local path (including filename) where the file
                                 should be saved. (e.g., 'my_downloaded_image.jpg' or 'data/archive.zip')

    Returns:
        str: The path to the downloaded file if successful, None otherwise.
    """
    try:
        # Ensure the output directory exists
        output_dir = os.path.dirname(destination_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"Created directory: {output_dir}")

        print(f"Attempting to download file to: {destination_path}")
        output = gdown.download(url=file_id_or_url, output=destination_path, quiet=False, fuzzy=True)

        if output and os.path.exists(output):
            print(f"File downloaded successfully: {output}")
            return output
        else:
            # gdown might return None or the path even if it fails internally for some large files before completion
            # So, an explicit check if the file exists and has size.
            if os.path.exists(destination_path) and os.path.getsize(destination_path) > 0:
                print(f"File downloaded successfully (verified): {destination_path}")
                return destination_path
            else:
                print(f"Download failed. Output path: {destination_path} may be incomplete or empty.")
                if os.path.exists(destination_path): # remove empty file if created
                    os.remove(destination_path)
                return None

    except Exception as e:
        print(f"An error occurred: {e}")
        if os.path.exists(destination_path): # Clean up partial file if error occurred
            if os.path.getsize(destination_path) == 0 : # remove if empty
                os.remove(destination_path)
        return None


def apply_text(
    prs: Presentation,
    slide_index: int,
    shape_index: int,
    text: str,
    font_name: str = "Calibri",
    font_size: int = 14,
    font_italic: bool = False,
    font_color = RGBColor(0, 0, 0), # black
    bold_items: Union[None, List[str]] = None,
) -> None:

    # retrieve shape
    shape = prs.slides[slide_index].shapes[shape_index]

    # write text to shape
    shape.text = text

    # apply formatting
    for paragraph in shape.text_frame.paragraphs:
        for run in paragraph.runs:
            run.font.name   = font_name
            run.font.size   = Pt(font_size)
            run.font.italic = font_italic
            run.font.color.rgb = font_color

            if bold_items is not None:
                for item in bold_items:
                    if run.text.startswith(item):
                        run.font.bold = True


def apply_figure(
    prs: Presentation,
    slide_index: int,
    shape_index: int,
    image_url: str,
) -> None:

    # download image & get its size
    resp = requests.get(image_url)
    resp.raise_for_status()
    img_bytes = BytesIO(resp.content)
    img = Image.open(img_bytes)
    orig_w, orig_h = img.size
    
    # open PPT and grab the target shape
    slide = prs.slides[slide_index]
    shape = slide.shapes[shape_index]
    
    # record the shape’s position & size
    left, top, w, h = shape.left, shape.top, shape.width, shape.height
    
    # compute scaling to fit entire image within the rectangle
    scale = min(w / orig_w, h / orig_h)
    scaled_w = int(orig_w * scale)
    scaled_h = int(orig_h * scale)
    
    # compute offsets to center the image in the rectangle
    offset_left = left + (w  - scaled_w) // 2
    offset_top = top  + (h  - scaled_h) // 2
    
    # insert the picture at the computed size & position
    img_bytes.seek(0)
    pic = slide.shapes.add_picture(
        img_bytes,
        offset_left,
        offset_top,
        width=scaled_w,
        height=scaled_h
    )
    
    # hide the original rectangle so only the image shows
    shape.fill.background()
    if shape.line:
        shape.line.fill.background()


def apply_table_cell(
    prs: Presentation,
    slide_index: int,
    shape_index: int,
    row_index: int,
    column_index: int,
    text: str,
    font_name: str = "Calibri",
    font_size: int = 14,
    font_italic: bool = False,
    font_color = RGBColor(0, 0, 0), # black
) -> None:

    # get shape object
    shape = prs.slides[slide_index].shapes[shape_index]

    # get cell
    cell = shape.table.cell(row_index, column_index)

    # apply text
    cell.text = text

    # apply font size to every run in that cell
    for paragraph in cell.text_frame.paragraphs:
        for run in paragraph.runs:
            run.font.name   = font_name
            run.font.size   = Pt(font_size)
            run.font.italic = font_italic
            run.font.color.rgb = font_color    


def fetch_github_release(state: GraphState) -> Dict:
    """
    Fetch the latest GitHub release tag and store it as current_version.
    """
    logging.info("--- Node: fetch_github_release ---")

    extracted = state.get('extracted_info', {}) or {}
    errors = state.get('error_messages', []) or []
    git_url = state.get('github_url')

    if not git_url:
        errors.append("No GitHub URL provided for latest-release lookup.")
        return {"extracted_info": extracted, "error_messages": errors}

    try:
        # turn "https://github.com/owner/repo" into "owner/repo"
        owner_repo = git_url.rstrip("/").split("github.com/")[1]
        api_url = f"https://api.github.com/repos/{owner_repo}/releases/latest"

        resp = requests.get(api_url, timeout=10)
        # Handle forbidden access by setting N/A
        if resp.status_code == 403:
            logging.warning("GitHub release lookup forbidden (403); setting current_version to 'N/A'")
            extracted["current_version"] = "N/A"
            return {"release_info": extracted, "github_release_errors": errors}
        resp.raise_for_status()
        data = resp.json()

        tag = data.get("tag_name")
        if tag:
            previous = extracted.get("current_version")
            # Override any existing non-default version
            if previous and previous != tag and previous != "N/A":
                logging.info(f"Overriding existing current_version '{previous}' with latest release '{tag}'")
            extracted["current_version"] = tag
            logging.info(f"Retrieved latest release tag: {tag}")
        else:
            logging.warning("No tag_name field in GitHub response.")
            errors.append("GitHub release JSON missing tag_name.")
    except Exception as e:
        logging.warning(f"Failed to retrieve latest release: {e}")
        errors.append(f"Latest-release lookup error: {e}")

    return {"release_info": extracted, "github_release_errors": errors}


def fetch_github_contributors(state: GraphState) -> Dict:
    """
    Fetch the list of GitHub repository contributors and store their logins as key_contributors.
    """
    logging.info("--- Node: fetch_github_contributors ---")
    extracted = state.get('extracted_info', {}) or {}
    errors = state.get('error_messages', []) or []
    git_url = state.get('github_url')

    if not git_url:
        errors.append("No GitHub URL provided for contributors lookup.")
        return {"extracted_info": extracted, "error_messages": errors}

    try:
        # Extract "owner/repo" from the URL
        owner_repo = git_url.rstrip("/").split("github.com/")[1]
        api_url = f"https://api.github.com/repos/{owner_repo}/contributors"

        resp = requests.get(api_url, timeout=10)
        # Handle forbidden access by setting current_version to 'N/A'
        if resp.status_code == 403:
            logging.warning("GitHub contributors lookup forbidden (403); setting current_version to 'N/A'")
            extracted["current_version"] = "N/A"
            return {"contributors_info": extracted, "github_contributor_errors": errors}
        resp.raise_for_status()
        data = resp.json()

        # Extract login names along with actual names where available
        contributors = []
        for entry in data:
            login = entry.get("login")
            if not login:
                continue
            # Default to login if no real name found
            display_name = login
            try:
                user_api_url = entry.get("url")
                user_resp = requests.get(user_api_url, timeout=10)
                user_resp.raise_for_status()
                user_data = user_resp.json()
                name = user_data.get("name")
                if name:
                    display_name = name
            except Exception as e:
                logging.warning(f"Could not fetch user details for {login}: {e}")
            # Format as "Name (@login)" if name differs, otherwise just login
            if display_name and display_name.lower() != login.lower():
                contributors.append(f"{display_name} (@{login})")
            else:
                contributors.append(login)

        if contributors:
            previous = extracted.get("key_contributors")
            if previous and previous != "N/A":
                logging.info(f"Overriding existing key_contributors '{previous}' with fetched contributors list")
            extracted["key_contributors"] = contributors
            logging.info(f"Retrieved {len(contributors)} contributors")
        else:
            logging.warning("No contributors found in GitHub response.")
            errors.append("GitHub contributors JSON empty or missing logins.")
    except Exception as e:
        logging.warning(f"Failed to retrieve GitHub contributors: {e}")
        errors.append(f"Contributors lookup error: {e}")

    return {"contributors_info": extracted, "github_contributor_errors": errors}


def fetch_github_repository(
        state: GraphState,
        repository_clone_directory: str = "./data/temp_repo_clone",
        chunk_size: int = 1000,
        chunk_overlap: int = 150,
        file_extensions: List[str] = None,
):
    """
    Fetch and process documents from a GitHub repository.

    This function attempts to clone or fetch updates from a specified GitHub
    repository, then loads and splits documents from it using the
    RecursiveCharacterTextSplitter.

    Args:
        state (GraphState): The current graph state containing input URLs and error messages.
        repository_clone_directory (str): Directory path to clone or fetch the repository into.
        chunk_size (int): The maximum number of characters per document chunk.
        chunk_overlap (int): The number of overlapping characters between chunks.

    Returns:
        dict: A dictionary with the processed GitHub documents under "github_docs"
              and any encountered error messages under "error_messages".
    """
    logging.info("--- Node: fetch_github_repository ---")

    # Determine branch to use (default 'main', override via input_urls['github_branch'])
    branch = state.get('github_branch', 'main')

    github_url = state.get("github_url")
    errors = state.get('error_messages', []) or [] # Ensure errors list exists
    docs = None
    raw_docs = None
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

    if not github_url:
        errors.append("Missing GitHub URL.")
        return {"github_content": None, "error_messages": errors}

    try:
        if os.path.exists(repository_clone_directory):
            try:
                logging.info(f"Repository directory exists at {repository_clone_directory}. Attempting git fetch...")

                repo = git.Repo(repository_clone_directory)

                if not repo.remotes:
                     raise git.InvalidGitRepositoryError(f"{repository_clone_directory} exists but has no remotes.")
                
                logging.info("Fetching updates from all remotes (--all --prune)...")
                repo.git.fetch('--all', '--prune')

                logging.info("Git fetch complete.")
                loader = GitLoader(repo_path=repository_clone_directory, branch=branch)

                raw_docs = loader.load()

            except git.InvalidGitRepositoryError as e:
                 logging.error(f"Existing directory {repository_clone_directory} is not a valid Git repository: {e}. Re-cloning...")
                 errors.append(f"InvalidGitRepositoryError for existing dir: {str(e)}")
                 shutil.rmtree(repository_clone_directory)
                 raw_docs = None

            except git.GitCommandError as e:
                logging.error(f"Git fetch error: {e}")
                errors.append(f"Git fetch failed: {str(e)}. Check permissions/repo state.")
                logging.info("Attempting to load documents from current state despite fetch error...")

                try:
                    loader = GitLoader(repo_path=repository_clone_directory, branch=branch)
                    raw_docs = loader.load()

                except Exception as load_e:
                    logging.error(f"Failed to load docs after fetch error: {load_e}")
                    errors.append(f"Failed to load docs after GitCommandError: {str(load_e)}")
                    raw_docs = None

            except Exception as e:
                logging.error(f"Accessing existing repo failed: {e}. Attempting re-clone...")
                errors.append(f"Error accessing existing repo: {str(e)}")

                try:
                    shutil.rmtree(repository_clone_directory)

                except OSError as rm_e:
                    logging.error(f"Failed to remove problematic directory: {rm_e}")
                    errors.append(f"Failed to remove problematic directory: {str(rm_e)}")

                raw_docs = None

        # If the directory didn't exist OR we decided to re-clone
        if not os.path.exists(repository_clone_directory) or raw_docs is None and github_url:
             try:
                logging.info(f"Cloning repository {github_url} to {repository_clone_directory}...")
                loader = GitLoader(clone_url=github_url, repo_path=repository_clone_directory, branch=branch)
                raw_docs = loader.load()
                logging.info("Clone complete.")
                
             except Exception as e:
                 logging.error(f"Cloning GitHub repo failed: {e}")
                 errors.append(f"GitHub clone failed: {str(e)}")
                 raw_docs = None

    except Exception as e:

        tb_str = traceback.format_exc()
        logging.critical(f"CRITICAL ERROR in fetch_github_repository: {e}\n{tb_str}")
        errors.append(f"Critical fetch_github_repository error: {str(e)}")
        docs = None

    # Process raw_docs through filter node
    docs, errors = filter_by_file_type(raw_docs, file_extensions, text_splitter, errors)    
    
    # reduce the file content by removing any latex or html tags
    if docs:
        cleaned_docs = []
        for doc in docs:
            cleaned_text = clean_content(doc.page_content)
            cleaned_docs.append(Document(page_content=cleaned_text, metadata=doc.metadata))
        docs = cleaned_docs

    # Log token count
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        num_tokens = len(encoding.encode("\n\n".join(doc.page_content for doc in docs)))
        logging.info(f"--- Node fetch_github_repository -- GitHub content requires {num_tokens} tokens")
    except Exception as e:
        logging.warning(f"Failed to count tokens for docs_text: {e}")


    return {"github_content": docs, "error_messages": errors}


# New node: fetch_website
def fetch_website(
        state: GraphState,
        timeout: int = 30,
        chunk_size: int = 1000,
        chunk_overlap: int = 150,
        max_depth: int = 3,
    ) -> Dict:
    """
    Fetch and process documents from a generic website.
    """
    logging.info("--- Node: fetch_website ---")

    errors: List[str] = []
    url = state.get("website_url")

    if not url:
        errors.append("No website URL provided.")
        return {"website_content": None, "website_errors": errors}
    
    try:
        # Load and split website content into Document chunks
        logging.info(f"Fetching website content from {url}")

        loader = RecursiveUrlLoader(
            url=url,
            max_depth=max_depth,
            prevent_outside=True,
            use_async=False,
            timeout=timeout,
        )
        
        raw_docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        docs = splitter.split_documents(raw_docs)
        logging.info(f"Processed {len(docs)} docs from website.")

    except Exception as e:
        logging.error(f"Failed to fetch or process website content: {e}")
        errors.append(f"Website fetch error: {e}")
        docs = None

    # reduce the file content by removing any latex or html tags
    if docs:
        cleaned_docs = []
        for doc in docs:
            cleaned_text = clean_content(doc.page_content)
            cleaned_docs.append(Document(page_content=cleaned_text, metadata=doc.metadata))
        docs = cleaned_docs

    # Log token count
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        num_tokens = len(encoding.encode("\n\n".join(doc.page_content for doc in docs)))
        logging.info(f"--- Node fetch_website -- Website content requires {num_tokens} tokens")
    except Exception as e:
        logging.warning(f"Failed to count tokens for docs_text: {e}")

    return {"website_content": docs, "website_errors": errors}


def fetch_publications_pdfs(state: GraphState) -> Dict:
    """
    Load local PDF files from a user-specified directory and extract their text for refinement.
    """
    logging.info("--- Node: fetch_publications_pdfs ---")

    errors = state.get('publication_errors', []) or []
    pdf_docs = []

    # Expect 'pdf_dir' in input_urls to specify where to load PDFs from
    pdf_dir = state.get("publication_directory")

    if not pdf_dir:
        errors.append("Missing PDF directory for publications ('pdf_dir' not provided).")
        return {"publication_content": [], "publication_errors": errors}
    
    if not os.path.isdir(pdf_dir):
        errors.append(f"PDF directory does not exist: {pdf_dir}")
        return {"publication_content": [], "publication_errors": errors}
    
    # Process each PDF file in the directory
    for fname in os.listdir(pdf_dir):
        if fname.lower().endswith('.pdf'):
            path = os.path.join(pdf_dir, fname)

            try:
                with pdfplumber.open(path) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text() or ""
                        text += "\n\n"

                collected_content = Document(page_content=text, metadata={'source': path})   
                pdf_docs.append(collected_content)

            except Exception as e:
                logging.error(f"Error processing PDF {path}: {e}")
                errors.append(str(e))

    return {"publication_content": pdf_docs, "publication_errors": errors}





# New node: extract_github_info
def extract_github_info(state: GraphState) -> Dict:
    """
    Extract information for the ModelCard from GitHub content using an LLM.
    """
    logging.info("--- Node: extract_github_info ---")

    docs = state.get("github_content") or []
    extracted = state.get("github_extracted_info", {}) or {}
    errors = state.get("github_repository_errors", []) or []

    extracted: Dict = {}

    # Prepare document text
    docs_text = "\n\n".join(doc.page_content for doc in docs)

    # Dynamically build list of field descriptions from the Pydantic ModelCard
    field_descriptions = "\n".join(
        f"- {name}: {field.description}"
        for name, field in ModelCard.model_fields.items()
    )
    
    prompt_template = f"""
You are an assistant that extracts metadata for a model card. The model card fields have the following names and descriptions:
{field_descriptions}

Given the following repository documents, extract these fields and output as a JSON object with the exact field names:

Repository documents content:
{{docs_text}}

JSON Output:
"""
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm | StrOutputParser()

    try:
        result_str = chain.invoke({"docs_text": docs_text})

        # Handle empty or whitespace-only output
        if not result_str or not result_str.strip():
            logging.warning("Empty output from LLM during information extraction")
            errors.append("Empty LLM output for information extraction")
            extracted = {}
            return {"github_extracted_info": extracted, "github_repository_errors": errors}
        
        try:
            # Remove Markdown code fences if present
            cleaned = re.sub(r"```(?:json)?\s*", "", result_str)
            cleaned = re.sub(r"```", "", cleaned).strip()
            extracted = json.loads(cleaned)

        except JSONDecodeError as jde:
            logging.error(f"JSON parsing error during information extraction: {jde}")
            errors.append(f"JSON parsing error: {jde}. Raw output: {cleaned!r}")
            # Attempt to extract JSON substring
            match = re.search(r'(\{.*\})', cleaned, re.DOTALL)
            if match:
                try:
                    extracted = json.loads(match.group(1))
                except JSONDecodeError as jde2:
                    logging.error(f"Secondary JSON parsing error: {jde2}")
                    errors.append(f"Secondary JSON parsing error: {jde2}. Substring: {match.group(1)!r}")
                    extracted = {}
            else:
                extracted = {}

    except Exception as e:
        logging.error(f"Error during information extraction: {e}")
        errors.append(str(e))

    return {"github_extracted_info": extracted, "github_repository_errors": errors}

# New node: extract_website_info
def extract_website_info(state: GraphState) -> Dict:
    """
    Extract information for the ModelCard from website content using an LLM.
    """
    logging.info("--- Node: extract_website_info ---")
    
    docs = state.get("website_content") or []
    extracted = state.get("website_extracted_info", {}) or {}
    errors = state.get("website_errors", []) or []
    extracted: Dict = {}

    # Prepare document text
    docs_text = "\n\n".join(doc.page_content for doc in docs)

    # Dynamically build list of field descriptions from the Pydantic ModelCard
    field_descriptions = "\n".join(
        f"- {name}: {field.description}"
        for name, field in ModelCard.model_fields.items()
    )

    prompt_template = f"""
You are an assistant that extracts metadata for a model card. The model card fields have the following names and descriptions:
{field_descriptions}

Given the following website documents, extract these fields and output as a JSON object with the exact field names:

Website documents content:
{{docs_text}}

JSON Output:
"""
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm | StrOutputParser()

    try:
        result_str = chain.invoke({"docs_text": docs_text})

        # Handle empty or whitespace-only output
        if not result_str or not result_str.strip():
            logging.warning("Empty output from LLM during website information extraction")
            errors.append("Empty LLM output for website information extraction")
            extracted = {}
            return {"website_extracted_info": extracted, "website_errors": errors}
        
        try:
            # Remove Markdown code fences if present
            cleaned = re.sub(r"```(?:json)?\s*", "", result_str)
            cleaned = re.sub(r"```", "", cleaned).strip()
            extracted = json.loads(cleaned)

        except JSONDecodeError as jde:
            logging.error(f"JSON parsing error during website information extraction: {jde}")
            errors.append(f"JSON parsing error: {jde}. Raw output: {cleaned!r}")
            # Attempt to extract JSON substring
            match = re.search(r'(\{.*\})', cleaned, re.DOTALL)
            if match:
                try:
                    extracted = json.loads(match.group(1))
                except JSONDecodeError as jde2:
                    logging.error(f"Secondary JSON parsing error: {jde2}")
                    errors.append(f"Secondary JSON parsing error: {jde2}. Substring: {match.group(1)!r}")
                    extracted = {}
            else:
                extracted = {}

    except Exception as e:
        logging.error(f"Error during website information extraction: {e}")
        errors.append(str(e))

    return {"website_extracted_info": extracted, "website_errors": errors}


# New node: extract_publication_info
def extract_publication_info(state: GraphState) -> Dict:
    """
    Extract information for the ModelCard from local publication PDFs using an LLM.
    """
    logging.info("--- Node: extract_publication_info ---")
    docs = state.get("publication_content") or []
    extracted: Dict = {}
    errors = state.get("publication_errors", []) or []

    # Prepare document text
    docs_text = "\n\n".join(doc.page_content for doc in docs)

    # Build field descriptions from ModelCard
    field_descriptions = "\n".join(
        f"- {name}: {field.description}"
        for name, field in ModelCard.model_fields.items()
    )

    prompt_template = f"""
You are an assistant that extracts metadata for a model card. The model card fields have the following names and descriptions:
{field_descriptions}

Given the following publication document texts, extract these fields and output as a JSON object with the exact field names:

Publication documents content:
{{docs_text}}

JSON Output:
"""
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm | StrOutputParser()

    try:
        result_str = chain.invoke({"docs_text": docs_text})

        if not result_str or not result_str.strip():
            logging.warning("Empty LLM output for publication information extraction")
            errors.append("Empty LLM output for publication information extraction")
            return {"publication_extracted_info": {}, "publication_errors": errors}

        # Clean and parse
        cleaned = re.sub(r"```(?:json)?\s*", "", result_str)
        cleaned = re.sub(r"```", "", cleaned).strip()
        try:
            extracted = json.loads(cleaned)
        except JSONDecodeError as jde:
            logging.error(f"JSON parsing error for publications: {jde}")
            errors.append(f"JSON parsing error: {jde}")
            extracted = {}
    except Exception as e:
        logging.error(f"Error during publication information extraction: {e}")
        errors.append(str(e))

    return {"publication_extracted_info": extracted, "publication_errors": errors}


def clean_content(text: str) -> str:
    """
    Remove HTML tags and LaTeX expressions from text.
    """
    # Remove HTML tags
    cleaned = re.sub(r'<[^>]+>', '', text)
    # Remove inline LaTeX math ($...$)
    cleaned = re.sub(r'\$.*?\$', '', cleaned)
    # Remove LaTeX commands like \command{...}
    cleaned = re.sub(r'\\[a-zA-Z]+\{.*?\}', '', cleaned)

    return cleaned


# Helper function: filter_by_file_type
def filter_by_file_type(
    raw_docs: Optional[List[Document]],
    file_extensions: List[str],
    text_splitter: RecursiveCharacterTextSplitter,
    errors: List[str],
) -> Tuple[Optional[List[Document]], List[str]]:
    """
    Convert PDFs to text, filter by allowed extensions (and LICENSE files),
    then split into document chunks.
    """

    if not raw_docs:
        logging.warning("No documents to filter or process.")
        return None, errors

    if file_extensions is None:
        file_extensions = [
            '.txt', '.md', '.rst', '.png', '.jpg', '.svg',
            '.cff', '.json', '.yaml', '.sh', '.cfg', '.config',
            '.cmake', '.xml', 'makefile', '.rd', '.rmd', '.toml',
            '.poetry', '.tex', '.bib', ''
        ]

    # 1) Convert any PDFs to plain-text Documents
    converted = []
    for doc in raw_docs:
        source = doc.metadata.get('source', '')
        if source.lower().endswith('.pdf'):
            try:
                with pdfplumber.open(source) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text() or ""
                        text += "\n\n"
                converted.append(Document(page_content=text, metadata=doc.metadata))
            except Exception as e:
                logging.error(f"Error converting PDF {source}: {e}")
                errors.append(f"PDF conversion error for {source}: {e}")
        else:
            converted.append(doc)

    # 2) Filter by extension or LICENSE filenames
    filtered = []
    for doc in converted:
        source = doc.metadata.get('source', '')
        _, ext = os.path.splitext(source)
        name = os.path.basename(source).lower()
        if ext.casefold() in file_extensions or name in ("license", "license.md", "license.txt"):
            filtered.append(doc)

    # 3) Split into final document chunks
    docs = text_splitter.split_documents(filtered)

    logging.info(f"Processed {len(docs)} docs from GitHub.")
    return docs, errors



def handle_error(state: GraphState) -> Dict:
    """
    Handle errors encountered in the workflow by logging them and consolidating validation issues.

    Args:
        state (GraphState): The current graph state containing 'error_messages' and 'validation_issues'.

    Returns:
        dict: A dictionary with 'handled_errors' updated to include any validation issues.
    """
    logging.info("--- Node: handle_error ---")
    errors = state.get('error_messages', []) or []
    issues = state.get('validation_issues', []) or []

    # Merge validation issues into error messages
    for issue in issues:
        errors.append(f"Validation issue: {issue}")

    logging.error(f"Workflow encountered errors: {errors}")

    return {"handled_errors": errors}


def build_model_card(state: GraphState) -> Dict:
    """
    Build the final model_card from extracted_info.
    """
    logging.info("--- Node: build_model_card ---")
    extracted = state.get('extracted_info', {}) or {}
    # Normalize values for model card compatibility
    normalized = {}
    for key, value in extracted.items():
        # Replace None with "N/A"
        if value is None:
            normalized[key] = "N/A"
        # Join lists for string-only fields
        elif key == 'systems_covered' and isinstance(value, list):
            normalized[key] = ", ".join(value)
        else:
            normalized[key] = value

    # Ensure figure path has a valid image extension
    fig = normalized.get("figure")
    if isinstance(fig, str):
        if not re.search(r'\.(png|jpe?g)$', fig, re.IGNORECASE):
            logging.info(f"Invalid figure extension for '{fig}', setting to N/A")
            normalized["figure"] = "N/A"

    return {"model_card": normalized}


def merge_info(state: GraphState) -> Dict:
    """
    Merge release_info and contributors_info into extracted_info and combine errors.
    """
    logging.info("--- Node: merge_info ---")

    # Card information extracted from various resources
    github_version = state.get("release_info", {}) or {}
    github_contributors = state.get("contributors_info", {}) or {}
    github_information = state.get("github_extracted_info", {}) or {}
    website_information = state.get("website_extracted_info", {}) or {}

    # Errors logged for each node
    github_repository_errors = state.get("github_repository_errors", []) or []
    github_release_errors = state.get("github_release_errors", []) or []
    github_contributor_errors = state.get("github_contributor_errors", []) or []
    website_errors = state.get("website_errors", []) or []

    # Combined error logs
    merged_errors = github_release_errors + github_contributor_errors + github_repository_errors + website_errors

    merged: Dict = {}

    # 1 & 2: Apply GitHub version and contributors with priority
    if github_version.get("current_version") and github_version.get("current_version") != "N/A":
        merged["current_version"] = github_version["current_version"]

    if github_contributors.get("key_contributors") and github_contributors.get("key_contributors") != "N/A":
        merged["key_contributors"] = github_contributors["key_contributors"]

    # 3: Merge GitHub-derived information, respecting version/contributors priority
    for key, value in github_information.items():
        if key == "current_version" and merged.get("current_version") is not None:
            continue
        if key == "key_contributors" and merged.get("key_contributors") is not None:
            continue
        merged[key] = value

    # 4: Merge website information by appending new values
    for key, value in website_information.items():
        
        if key in ("current_version", "key_contributors"):
            continue
        
        existing = merged.get(key)
        if existing is None:
            merged[key] = value
        else:
            # Combine list and list
            if isinstance(existing, list) and isinstance(value, list):
                merged[key] = list(dict.fromkeys(existing + value))
            # Existing list, new string – keep list semantics
            elif isinstance(existing, list) and isinstance(value, str):
                if value not in existing:
                    merged[key] = existing + [value]
            # Existing string, new list – append each new item after a period
            elif isinstance(existing, str) and isinstance(value, list):
                text = existing.rstrip('.')
                for v in value:
                    if v and v not in text:
                        text = f"{text}. {v}"
                merged[key] = text
            # Both strings – append new string after a period
            elif isinstance(existing, str) and isinstance(value, str):
                if existing == value:
                    merged[key] = existing
                else:
                    merged[key] = f"{existing.rstrip('.')}. {value}"

    # 5: Merge publication information by appending new values
    publication_information = state.get("publication_extracted_info", {}) or {}
    for key, value in publication_information.items():
        if key in ("current_version", "key_contributors"):
            continue
        existing = merged.get(key)
        if existing is None:
            merged[key] = value
        else:
            # Combine list and list
            if isinstance(existing, list) and isinstance(value, list):
                merged[key] = list(dict.fromkeys(existing + value))
            # Existing list, new string – keep list semantics
            elif isinstance(existing, list) and isinstance(value, str):
                if value not in existing:
                    merged[key] = existing + [value]
            # Existing string, new list – append each new item after a period
            elif isinstance(existing, str) and isinstance(value, list):
                text = existing.rstrip('.')
                for v in value:
                    if v and v not in text:
                        text = f"{text}. {v}"
                merged[key] = text
            # Both strings – append new string after a period
            elif isinstance(existing, str) and isinstance(value, str):
                if existing == value:
                    merged[key] = existing
                else:
                    merged[key] = f"{existing.rstrip('.')}. {value}"

    return {"raw_extracted_info": merged, "error_messages": merged_errors}


def remove_redundancy(state: GraphState) -> Dict:
    """
    Use an LLM to remove redundant or mismatched entries based on ModelCard field descriptions.
    """
    logging.info("--- Node: remove_redundancy ---")
    raw = state.get("raw_extracted_info", {}) or {}
    errors = state.get("error_messages", []) or []

    # Build field descriptions for prompting
    field_lines = "\n".join(
        f"- {name}: {field.description}"
        for name, field in ModelCard.model_fields.items()
    )

    # Prepare JSON payload
    raw_json = json.dumps(raw)

    prompt = ChatPromptTemplate.from_template(
        """You are given raw extracted metadata for a model card and the official field descriptions:
{fields}

Raw metadata (JSON):
{raw}

Please return a JSON object containing only the cleaned metadata: 
- Remove any duplicate or redundant entries.
- Ensure each value matches the corresponding field description.
- Omit any keys with values that do not match the description."""
    )

    formatted = prompt.format_prompt(fields=field_lines, raw=raw_json)
    llm_client = state.get("llm_client")
    try:
        ai_message = llm_client.invoke(formatted.to_messages())
        result_text = ai_message.content
        # Strip markdown fences if present
        cleaned_text = re.sub(r"```(?:json)?\s*", "", result_text).replace("```", "").strip()
        cleaned = json.loads(cleaned_text)
    except Exception as e:
        logging.error(f"LLM redundancy removal failed: {e}")
        errors.append(f"remove_redundancy error: {e}")
        cleaned = raw

    return {"extracted_info": cleaned}


if __name__ == "__main__":

    api_key = os.getenv("AZURE_OPENAI_API_KEY", default=None)
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", default=None)
    deployment = "gpt-4o"
    openai_api_version = "2024-02-01"

    # file to process
    forms_file = "/Users/d3y010/repos/github/modelcard/data/eesm_modelcard_responses.xlsx"

    df = pd.read_excel(forms_file)

    df.columns = [
        "timestamp",
        "submitter_email",
        "sponsoring_project",
        "capability_name",
        "capabiity_type",
        "code_repository",
        "branch",
        "data_repository",
        "additional_documents",
        "website"
    ]

    # PROCESS INDIVIDUAL ROW PER MODEL CARD
    llm = AzureChatOpenAI(
        model_name="gpt-4o", 
        temperature=0.1, 
        api_key=api_key,
        openai_api_version=openai_api_version,
        azure_deployment=deployment,
        azure_endpoint=endpoint,
    )

    # --- Build the Graph ---
    workflow = StateGraph(GraphState)

    # Add other nodes as before
    workflow.add_node("fetch_github_release", fetch_github_release)
    workflow.add_node("fetch_github_contributors", fetch_github_contributors)
    workflow.add_node("fetch_github_repository", fetch_github_repository)
    workflow.add_node("extract_github_info", extract_github_info)
    workflow.add_node("merge_info", merge_info)
    workflow.add_node("remove_redundancy", remove_redundancy)
    workflow.add_node("error_handler", handle_error)
    workflow.add_node("build_model_card", build_model_card)
    workflow.add_node("extract_website_info", extract_website_info)
    workflow.add_node("fetch_publications_pdfs", fetch_publications_pdfs)
    workflow.add_node("extract_publication_info", extract_publication_info)
    workflow.add_node("fetch_website", fetch_website)

    # Add entry points so they run as their own agents
    workflow.set_entry_point("fetch_github_release")
    workflow.set_entry_point("fetch_github_contributors")
    workflow.set_entry_point("fetch_github_repository")
    workflow.set_entry_point("fetch_website")
    workflow.set_entry_point("fetch_publications_pdfs")

    # Connect agents
    workflow.add_edge("fetch_website", "extract_website_info")
    workflow.add_edge("extract_website_info", "merge_info")
    workflow.add_edge("fetch_publications_pdfs", "extract_publication_info")
    workflow.add_edge("extract_publication_info", "merge_info")
    workflow.add_edge("fetch_github_release", "merge_info")
    workflow.add_edge("fetch_github_contributors", "merge_info")
    workflow.add_edge("fetch_github_repository", "extract_github_info")
    workflow.add_edge("extract_github_info", "merge_info")

    # Remove direct edges from merge_info to build_model_card and error_handler, instead go through dedupe_info
    workflow.add_edge("merge_info", "remove_redundancy")
    workflow.add_edge("remove_redundancy", "build_model_card")
    workflow.add_edge("remove_redundancy", "error_handler")
    workflow.add_edge("build_model_card", END)
    workflow.add_edge("error_handler", END)

    # Compile the graph
    app = workflow.compile()

    # download files if there are any
    if np.isnan(df.data_repository.iloc[0]):
        files_to_download = None 
    else:
        files_to_download = df.additional_documents.iloc[0].split(",")

    if files_to_download:
        for i in files_to_download:
            destination_url = f"{i.split("id=")[-1]}.pdf"
            downloaded_file_url = download_gdrive_public_file(i, destination_url)

    nm = df.capability_name.iloc[0]

    # Ensure publication directory exists
    pub_dir = f"./data/{nm}"
    if not os.path.exists(pub_dir):
        os.makedirs(pub_dir)

    inputs = {
        "github_url": df.code_repository.iloc[0],
        "github_branch": df.branch.iloc[0],
        "extracted_info": {},
        "raw_extracted_info": {},
        "error_messages": [],
        "validation_issues": {},
        "model_card": None, # Ensure model_card starts as None
        "release_info": {},
        "github_release_errors": [],
        "contributors_info": {},
        "github_contributor_errors": [],
        "github_extracted_info": {},
        "github_repository_errors": [],
        "handled_errors": [],
        "llm_client": llm,
        "website_url": df.website.iloc[0], 
        "website_content": None,
        "website_errors": [],
        "website_extracted_info": {},
        "publication_directory": f"./data/{nm}",
        "publication_content": None,
        "publication_extracted_info": {},
        "publication_errors": [],
    }

    print("Starting graph execution...")

    final_state = app.invoke(inputs)

    print("\n--- Graph Execution Finished ---")

    # Check final state for success
    final_card_data = final_state.get("model_card")
    final_issues = final_state.get("validation_issues")
    final_errors = final_state.get("error_messages")
    final_errors = final_state.get("handled_errors", final_state.get("error_messages"))

    # Define success more strictly: model_card exists AND no validation issues AND no critical errors?
    # For now, rely on the routing logic: if it reached END without errors/issues routing it away.
    # A simple check is if final_card_data exists and final_issues is empty.
    if final_card_data and not final_issues:
        print("Successfully generated Model Card:")
        try:
            # Create the final Pydantic object from the dictionary state
            final_card = ModelCard(**final_card_data)

            print(final_card.model_dump_json(indent=2))

        except Exception as e:
            print(f"Error creating/printing final ModelCard object: {e}")
            print("Raw dictionary data:", final_card_data)
    else:
        print("Failed to generate valid Model Card.")
        print("Final State Details:")
        if final_errors:
            print("Errors:", final_errors)
        if final_issues:
            print("Validation Issues:", final_issues)
        
        print("Partial Card Data:", final_card_data) # Uncomment to see potentially invalid data


    # Paths
    template_path = "./data/model_card_template.pptx"
    output_path   = "./data/model_card_filled.pptx"

    # Load the presentation
    prs = Presentation(template_path)

    description_slide_index = 0 # slide 1
    details_slide_index = 1 # slide 2

    project_logos = {
        "hyperfacets": "/Users/d3y010/repos/github/modelcard/data/project_logos/hyperfacets.png",
        "im3": "/Users/d3y010/repos/github/modelcard/data/project_logos/im3.png",
    }

    # from source inputs
    project_logos = "Sponsoring Project Logos"
    github_repo = final_card_data.get("github_url")

    # from AI agents
    capability_name = final_card_data.get("capability_name", None)
    brief_description = final_card_data.get("brief_description", None)
    contact_name = final_card_data.get("contact_name", "Contact Name")
    contact_email = final_card_data.get("contact_email", "Contact Email")
    current_version = final_card_data.get("current_version", None)
    license = final_card_data.get("license", None)

    # make this not include contact person
    key_contributors = final_card_data.get("key_contributors", None)
    key_contributors_additional = ", et al." if len(key_contributors) > 3 else ""

    doi = final_card_data.get("doi", None)
    category = ", ".join(final_card_data.get("category", [""]))
    systems_covered = final_card_data.get("systems_covered", "")
    computational_requirements = final_card_data.get("computational_requirements", "")
    spatial_resolution = ", ".join(final_card_data.get("spatial_resolution", ""))
    geographic_scope = final_card_data.get("geographic_scope", "")
    temporal_resolution = ", ".join(final_card_data.get("temporal_resolution", [""]))
    temporal_range = ", ".join(final_card_data.get("temporal_range", [""]))
    input_variables = ", ".join(final_card_data.get("input_variables", [""]))
    output_variables = ", ".join(final_card_data.get("output_variables", [""]))
    interdependencies = ", ".join(final_card_data.get("interdependencies", [""]))

    # limit publication number
    n_publications_max = 3
    key_publications = "\n".join(final_card_data.get("key_publications", [""])[:n_publications_max])

    # modify agent to only use figs on web
    # optional_figure_url = "https://climate.ucdavis.edu/TempestExtremesLogo.png"
    # figure_caption = "TempestExtremes provides algorithms implemented in C++ for tracking and characterizing tropical cyclones (TCs), extratropical cyclones (ETCs), monsoonal depressions, atmospheric blocks, atmospheric rivers, and mesoscale convective systems (MCSs)."

    # modify agent to select model, tool, or dataset
    capability_type = f"Tool (Current Version: {current_version})"


    capability_name_index = 1 
    brief_description_index = 2 
    contact_index = 3 
    project_logo_index = 4
    access_block_index = 5
    optional_figure_index = 6
    figure_caption_index = 7
    capability_type_index = 8
    current_version_index = 10

    # capability name
    apply_text(
        slide_index=description_slide_index,
        shape_index=capability_name_index,
        text=capability_name,
        font_size=38,
        font_italic=True
    )

    # capability type
    apply_text(
        slide_index=description_slide_index,
        shape_index=capability_type_index,
        text=capability_type,
        font_size=18,
        font_italic=True
    )

    # description
    brief_description_block = (
        "Brief Description:\n"
        f"{brief_description}"
    )

    apply_text(
        slide_index=description_slide_index,
        shape_index=brief_description_index,
        text=brief_description_block,
        font_size=18,
        font_italic=False,
        bold_items=["Brief Description"]

    )

    # contact and key contributors
    n_contributors_max = 3
    contact_block = (
        "Contact:\n"
        f"{contact_name}, {contact_email}\n\n"
        "Key Contributors:\n"
        f"{', '.join(key_contributors[:n_contributors_max])}{key_contributors_additional}"
    )

    apply_text(
        slide_index=description_slide_index,
        shape_index=contact_index,
        text=contact_block,
        font_size=14,
        font_italic=False,
        bold_items=["Contact", "Key Contributors"]
    )

    # repository and DOI
    access_block = (
        "\nRepository:\n"
        f"{github_repo}\n\n"
        "DOI:\n"
        f"{doi}\n\n"
        "License:\n"
        f"{license}\n"
    )

    apply_text(
        slide_index=description_slide_index,
        shape_index=access_block_index,
        text=access_block,
        font_size=13,
        font_italic=False,
        bold_items=["Repository", "DOI", "License"]
    )

    # figure
    apply_figure(
        slide_index=description_slide_index,
        shape_index=optional_figure_index,
        image_url=optional_figure_url
    )

    # figure caption
    apply_text(
        slide_index=description_slide_index,
        shape_index=figure_caption_index,
        text=figure_caption,
        font_size=13,
        font_italic=False,
    )

    details_capability_name_index = 1
    details_sponsoring_project_logos_index = 2
    details_capability_type_index = 3
    details_current_version_index = 3
    details_table_index = 5
    details_table_category = [0, 1]
    details_table_systems_covered = [1, 1]
    details_table_computational_requirements = [2, 1]
    details_table_spatial_resolution = [3, 1]
    details_table_geographic_scope = [4, 1]
    details_table_temporal_resolution = [5, 1]
    details_table_temporal_range = [6, 1]
    details_table_input_variables = [7, 1]
    details_table_output_variables = [8, 1]
    details_table_interdependencies = [9, 1]
    details_table_key_publications = [10, 1]

    # title
    apply_text(
        slide_index=details_slide_index,
        shape_index=details_capability_name_index,
        text=capability_name,
        font_size=38,
        font_italic=True
    )

    # capability type
    apply_text(
        slide_index=details_slide_index,
        shape_index=details_capability_type_index,
        text=capability_type,
        font_size=18,
        font_italic=True
    )

    # systems_covered
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_category[0],
        column_index=details_table_category[1],
        text=category
    )

    # systems_covered
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_systems_covered[0],
        column_index=details_table_systems_covered[1],
        text=systems_covered
    )

    # computational requirements
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_computational_requirements[0],
        column_index=details_table_computational_requirements[1],
        text=computational_requirements
    )

    # spatial_resolution
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_spatial_resolution[0],
        column_index=details_table_spatial_resolution[1],
        text=spatial_resolution
    )

    # geographic_scope
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_geographic_scope[0],
        column_index=details_table_geographic_scope[1],
        text=geographic_scope
    )

    # temporal_resolution
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_temporal_resolution[0],
        column_index=details_table_temporal_resolution[1],
        text=temporal_resolution
    )

    # temporal_range
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_temporal_range[0],
        column_index=details_table_temporal_range[1],
        text=temporal_range
    )

    # input_variables
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_input_variables[0],
        column_index=details_table_input_variables[1],
        text=input_variables
    )

    # output_variables
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_output_variables[0],
        column_index=details_table_output_variables[1],
        text=output_variables
    )

    # interdependencies
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_interdependencies[0],
        column_index=details_table_interdependencies[1],
        text=interdependencies
    )

    # key_publications
    apply_table_cell(
        slide_index=details_slide_index,
        shape_index=details_table_index,
        row_index=details_table_key_publications[0],
        column_index=details_table_key_publications[1],
        text=key_publications
    )
 
    prs.save(output_path)
